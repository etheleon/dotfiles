{
 "kernel_python_credentials" : {
 "username": "",
 "password": "",
 "url": "https://us.qubole.com/livy-spark-<cluster_id>"
 },

"kernel_scala_credentials" : {
 "username": "",
 "password": "",
 "url": "https://us.qubole.com/livy-spark-<cluster_id>"
 },

"logging_config": {
 "version": 1,
 "formatters": {
 "magicsFormatter": {
 "format": "%(asctime)s\t%(levelname)s\t%(message)s",
 "datefmt": ""
 }
 },
 "handlers": {
 "magicsHandler": {
 "class": "hdijupyterutils.filehandler.MagicsFileHandler",
 "formatter": "magicsFormatter",
 "home_path": "~/.sparkmagic"
 }
 },
 "loggers": {
 "magicsLogger": {
 "handlers": ["magicsHandler"],
 "level": "DEBUG",
 "propagate": 0
 }
 }
 },

"wait_for_idle_timeout_seconds": 15,
 "status_sleep_seconds": 2,
 "statement_sleep_seconds": 2,
 "livy_session_startup_timeout_seconds": 180,

"fatal_error_suggestion": "The code failed because of a fatal error:\n\t{}.\n\nSome things to try:\na) Make sure Spark has enough available resources for Jupyter to create a Spark context.\nb) Contact your Jupyter administrator to make sure the Spark magics library is configured correctly.\nc) Restart the kernel.",

"ignore_ssl_errors": false,

"session_configs": {
 "driverMemory": "1000M",
 "executorCores": 2
 },

"use_auto_viz": true,
 "max_results_sql": 2500,
 "pyspark_dataframe_encoding": "utf-8",

 "heartbeat_refresh_seconds": 5,
 "livy_server_heartbeat_timeout_seconds": 30,
 "heartbeat_retry_seconds": 2,

"server_extension_default_kernel_name": "pysparkkernel",
 "custom_headers": {
 "X-AUTH-TOKEN":"<auth token>"
 }
}
